{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defdc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1044ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/daniil/.cache/huggingface/datasets/csv/default-5d3942e87f7691cd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b97914d978c4afcb53fd687b84afaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 56355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisql_processed_dataset = load_dataset(\"csv\", data_files=\"datasets/eng2SQL_raw.csv\")\n",
    "\n",
    "wikisql_processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec129d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 52355\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_train_test = wikisql_processed_dataset[\"train\"].train_test_split(test_size=2000)\n",
    "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=2000)\n",
    "\n",
    "wikisql_processed_dataset[\"train\"] = datasets_train_validation[\"train\"]\n",
    "wikisql_processed_dataset[\"validation\"] = datasets_train_validation[\"test\"]\n",
    "wikisql_processed_dataset[\"test\"] = datasets_train_test[\"test\"]\n",
    "\n",
    "wikisql_processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bb04ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c965de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'human_sql'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only a subsample of the datasets\n",
    "wikisql_processed_dataset[\"train\"] = wikisql_processed_dataset[\"train\"].shuffle().select(range(50000))\n",
    "wikisql_processed_dataset[\"validation\"] = wikisql_processed_dataset[\"validation\"].shuffle().select(range(1000))\n",
    "wikisql_processed_dataset[\"test\"] = wikisql_processed_dataset[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "wikisql_processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364a793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"translate English to SQL: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [prefix + text for text in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"human_sql\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5c92c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/torch/graduation-work/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = wikisql_processed_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c280622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27904531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.28.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model_name = \"t5-small-english-to-sql-raw-translation\"\n",
    "model_dir = f\"models/{model_name}\"\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30ad7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5852/909098219.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"bleu\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "metric = load_metric(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, references = eval_pred\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    predictions = [pred.split() for pred in predictions]\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    references = np.where(references != -100, references, tokenizer.pad_token_id)\n",
    "    references = tokenizer.batch_decode(references, skip_special_tokens=True)\n",
    "    references = [ref.split() for ref in references]\n",
    "    references = [[ref] for ref in references]\n",
    "    \n",
    "    \n",
    "    # Compute BLEU scores\n",
    "    result = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7846f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns an untrained model to be trained\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4adcd949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start TensorBoard before training to monitor it in progress\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0f8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/torch/graduation-work/venv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 1:16:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Precisions</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.279200</td>\n",
       "      <td>1.600417</td>\n",
       "      <td>0.181706</td>\n",
       "      <td>[0.5535853685736589, 0.3063888540228442, 0.19865078225922206, 0.10631895687061184]</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.770758</td>\n",
       "      <td>8967</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.674600</td>\n",
       "      <td>1.236402</td>\n",
       "      <td>0.346293</td>\n",
       "      <td>[0.6930783242258652, 0.47613150191398335, 0.35705225773718924, 0.24803950043566658]</td>\n",
       "      <td>0.837535</td>\n",
       "      <td>0.849407</td>\n",
       "      <td>9882</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.367100</td>\n",
       "      <td>1.067369</td>\n",
       "      <td>0.390736</td>\n",
       "      <td>[0.7549382716049383, 0.5360091743119266, 0.4134403729120808, 0.3062741599762117]</td>\n",
       "      <td>0.821262</td>\n",
       "      <td>0.835482</td>\n",
       "      <td>9720</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.208700</td>\n",
       "      <td>0.967295</td>\n",
       "      <td>0.421017</td>\n",
       "      <td>[0.7730445493738453, 0.5666895447266072, 0.44894797986317286, 0.3474011550422035]</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>0.837373</td>\n",
       "      <td>9742</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.069500</td>\n",
       "      <td>0.897922</td>\n",
       "      <td>0.444973</td>\n",
       "      <td>[0.7899117020196894, 0.5882751609623856, 0.47021384928716903, 0.36973319725907566]</td>\n",
       "      <td>0.834638</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>9853</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>0.844253</td>\n",
       "      <td>0.465431</td>\n",
       "      <td>[0.7991504854368932, 0.6073357335733574, 0.49359868170870835, 0.3969521044992743]</td>\n",
       "      <td>0.838134</td>\n",
       "      <td>0.849923</td>\n",
       "      <td>9888</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.949100</td>\n",
       "      <td>0.807839</td>\n",
       "      <td>0.479121</td>\n",
       "      <td>[0.8077895801719778, 0.6227349465391109, 0.5101445599797109, 0.4167271671264702]</td>\n",
       "      <td>0.837834</td>\n",
       "      <td>0.849665</td>\n",
       "      <td>9885</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.769996</td>\n",
       "      <td>0.489040</td>\n",
       "      <td>[0.8133400707427993, 0.6345137717818999, 0.5213426219126029, 0.4293793503480278]</td>\n",
       "      <td>0.838832</td>\n",
       "      <td>0.850524</td>\n",
       "      <td>9895</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>0.493899</td>\n",
       "      <td>[0.81591737545565, 0.6411671924290221, 0.5298374809547994, 0.4375454413261597]</td>\n",
       "      <td>0.836936</td>\n",
       "      <td>0.848891</td>\n",
       "      <td>9876</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.726063</td>\n",
       "      <td>0.499194</td>\n",
       "      <td>[0.8192136197811106, 0.6472710870545783, 0.5374936451448907, 0.44577085456398313]</td>\n",
       "      <td>0.836137</td>\n",
       "      <td>0.848204</td>\n",
       "      <td>9868</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>0.708497</td>\n",
       "      <td>0.501189</td>\n",
       "      <td>[0.8186957399555824, 0.6472041320458118, 0.5365545155578042, 0.44592442449688724]</td>\n",
       "      <td>0.839927</td>\n",
       "      <td>0.851470</td>\n",
       "      <td>9906</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.690289</td>\n",
       "      <td>0.509844</td>\n",
       "      <td>[0.8221774193548387, 0.6552690582959642, 0.5467171717171717, 0.4578818089871406]</td>\n",
       "      <td>0.841321</td>\n",
       "      <td>0.852673</td>\n",
       "      <td>9920</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.683469</td>\n",
       "      <td>0.515916</td>\n",
       "      <td>[0.8280998389694042, 0.661705461056401, 0.5525453629032258, 0.46352364475201846]</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.854048</td>\n",
       "      <td>9936</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.661342</td>\n",
       "      <td>0.517898</td>\n",
       "      <td>[0.8268167655040708, 0.6623086378366297, 0.5544093596678827, 0.4665419484817959]</td>\n",
       "      <td>0.844202</td>\n",
       "      <td>0.855166</td>\n",
       "      <td>9949</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.653448</td>\n",
       "      <td>0.525060</td>\n",
       "      <td>[0.8318423962207258, 0.6696837635489998, 0.5630896968172097, 0.47704705713052237]</td>\n",
       "      <td>0.844202</td>\n",
       "      <td>0.855166</td>\n",
       "      <td>9949</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.772600</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>0.527749</td>\n",
       "      <td>[0.8334172625642059, 0.6753275842759547, 0.56930256022197, 0.4811661134362823]</td>\n",
       "      <td>0.842216</td>\n",
       "      <td>0.853447</td>\n",
       "      <td>9929</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.775700</td>\n",
       "      <td>0.639276</td>\n",
       "      <td>0.531609</td>\n",
       "      <td>[0.8360688752391502, 0.6785354383607659, 0.5735720590089523, 0.4873755590823835]</td>\n",
       "      <td>0.842414</td>\n",
       "      <td>0.853619</td>\n",
       "      <td>9931</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.722900</td>\n",
       "      <td>0.630019</td>\n",
       "      <td>0.530266</td>\n",
       "      <td>[0.8313760915386932, 0.6749972107553275, 0.5698857214617606, 0.4835559385322418]</td>\n",
       "      <td>0.845590</td>\n",
       "      <td>0.856369</td>\n",
       "      <td>9963</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.623156</td>\n",
       "      <td>0.533873</td>\n",
       "      <td>[0.8363508348420841, 0.6788190561395661, 0.5754218081087887, 0.49121290694324404]</td>\n",
       "      <td>0.843507</td>\n",
       "      <td>0.854564</td>\n",
       "      <td>9942</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.746100</td>\n",
       "      <td>0.612541</td>\n",
       "      <td>0.539862</td>\n",
       "      <td>[0.8380684670213834, 0.6848566008258007, 0.5820876774274588, 0.49777330843269646]</td>\n",
       "      <td>0.845392</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>9961</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>0.600737</td>\n",
       "      <td>0.541223</td>\n",
       "      <td>[0.8367428800641797, 0.6851315202853322, 0.5835423983943804, 0.4995697074010327]</td>\n",
       "      <td>0.846482</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>9972</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.740200</td>\n",
       "      <td>0.596749</td>\n",
       "      <td>0.543753</td>\n",
       "      <td>[0.8368104312938817, 0.6875139353400223, 0.5870765370138018, 0.5045911047345768]</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>0.856971</td>\n",
       "      <td>9970</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>0.591626</td>\n",
       "      <td>0.542865</td>\n",
       "      <td>[0.8369805259987955, 0.6870118277170274, 0.5864104496357699, 0.5040218328066648]</td>\n",
       "      <td>0.845491</td>\n",
       "      <td>0.856283</td>\n",
       "      <td>9962</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.590509</td>\n",
       "      <td>0.548132</td>\n",
       "      <td>[0.8385447985568251, 0.6913566495878815, 0.5926297317623465, 0.5103181427343079]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.581265</td>\n",
       "      <td>0.549359</td>\n",
       "      <td>[0.8398476648626979, 0.6925818667854756, 0.5936324893457007, 0.5123244482659788]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.576092</td>\n",
       "      <td>0.549625</td>\n",
       "      <td>[0.8394665597112203, 0.6935250195029533, 0.5943810359964882, 0.5134088627563459]</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>9973</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.564586</td>\n",
       "      <td>0.549905</td>\n",
       "      <td>[0.8378756114605171, 0.6911389597427082, 0.590994137457902, 0.5096194955109021]</td>\n",
       "      <td>0.850930</td>\n",
       "      <td>0.861011</td>\n",
       "      <td>10017</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.565957</td>\n",
       "      <td>0.552351</td>\n",
       "      <td>[0.8377784437119169, 0.6929308622794362, 0.594931968543253, 0.5154756810726002]</td>\n",
       "      <td>0.850338</td>\n",
       "      <td>0.860495</td>\n",
       "      <td>10011</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.564626</td>\n",
       "      <td>0.556311</td>\n",
       "      <td>[0.8418846077061289, 0.6979374584165003, 0.5991519082065353, 0.518666286691365]</td>\n",
       "      <td>0.851029</td>\n",
       "      <td>0.861097</td>\n",
       "      <td>10018</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.562902</td>\n",
       "      <td>0.554428</td>\n",
       "      <td>[0.8416942024632021, 0.6973406030933571, 0.5994741454864154, 0.5193931587233434]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.557324</td>\n",
       "      <td>0.556278</td>\n",
       "      <td>[0.8436498397435898, 0.70013357079252, 0.6019539078156313, 0.5216208476517755]</td>\n",
       "      <td>0.847670</td>\n",
       "      <td>0.858174</td>\n",
       "      <td>9984</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>0.552370</td>\n",
       "      <td>0.558254</td>\n",
       "      <td>[0.8456907795725895, 0.7035797925727668, 0.605623195682189, 0.5261949189034018]</td>\n",
       "      <td>0.845986</td>\n",
       "      <td>0.856713</td>\n",
       "      <td>9967</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.547858</td>\n",
       "      <td>0.560429</td>\n",
       "      <td>[0.8433530059017705, 0.7025675225075025, 0.607102663498812, 0.5279405459482636]</td>\n",
       "      <td>0.848955</td>\n",
       "      <td>0.859292</td>\n",
       "      <td>9997</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.543244</td>\n",
       "      <td>0.560816</td>\n",
       "      <td>[0.8445310937812438, 0.7027327260608753, 0.6069732566858286, 0.5274207369323051]</td>\n",
       "      <td>0.849449</td>\n",
       "      <td>0.859722</td>\n",
       "      <td>10002</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.543312</td>\n",
       "      <td>0.558778</td>\n",
       "      <td>[0.8430331563658219, 0.7021039741734387, 0.6057872980082676, 0.5268509236717743]</td>\n",
       "      <td>0.847571</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>9983</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.537912</td>\n",
       "      <td>0.562932</td>\n",
       "      <td>[0.8468865938032688, 0.7071213640922769, 0.6119402985074627, 0.5334863043166499]</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>9973</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.534167</td>\n",
       "      <td>0.562407</td>\n",
       "      <td>[0.8446398241582576, 0.703962703962704, 0.6080659258334374, 0.5297474675417321]</td>\n",
       "      <td>0.850140</td>\n",
       "      <td>0.860323</td>\n",
       "      <td>10009</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.530276</td>\n",
       "      <td>0.564926</td>\n",
       "      <td>[0.8471694338867773, 0.7086019115358969, 0.6122780695173794, 0.5332190312901843]</td>\n",
       "      <td>0.849054</td>\n",
       "      <td>0.859378</td>\n",
       "      <td>9998</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.527573</td>\n",
       "      <td>0.566059</td>\n",
       "      <td>[0.8480367669097812, 0.7086247086247086, 0.6123111499562992, 0.534170352404052]</td>\n",
       "      <td>0.850140</td>\n",
       "      <td>0.860323</td>\n",
       "      <td>10009</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.610400</td>\n",
       "      <td>0.524629</td>\n",
       "      <td>0.567908</td>\n",
       "      <td>[0.848381941669996, 0.7102751886373724, 0.61445332001997, 0.5370792926411866]</td>\n",
       "      <td>0.850436</td>\n",
       "      <td>0.860581</td>\n",
       "      <td>10012</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.522417</td>\n",
       "      <td>0.566517</td>\n",
       "      <td>[0.8488488488488488, 0.7100111234705228, 0.6141426783479349, 0.5374821173104435]</td>\n",
       "      <td>0.848263</td>\n",
       "      <td>0.858690</td>\n",
       "      <td>9990</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.515334</td>\n",
       "      <td>0.569807</td>\n",
       "      <td>[0.8524952304448238, 0.7154816385757339, 0.6210579218494786, 0.5453369737031183]</td>\n",
       "      <td>0.845194</td>\n",
       "      <td>0.856025</td>\n",
       "      <td>9959</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.515517</td>\n",
       "      <td>0.573195</td>\n",
       "      <td>[0.8509490509490509, 0.7155382907880133, 0.6217228464419475, 0.5456490727532097]</td>\n",
       "      <td>0.850239</td>\n",
       "      <td>0.860409</td>\n",
       "      <td>10010</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.648400</td>\n",
       "      <td>0.514599</td>\n",
       "      <td>0.568684</td>\n",
       "      <td>[0.8510169321711252, 0.7135062910589022, 0.6179676732239068, 0.5406102277610657]</td>\n",
       "      <td>0.847373</td>\n",
       "      <td>0.857916</td>\n",
       "      <td>9981</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.514330</td>\n",
       "      <td>0.573008</td>\n",
       "      <td>[0.8523261630815407, 0.7175097276264591, 0.6220137585991244, 0.5461043602573267]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.508502</td>\n",
       "      <td>0.573946</td>\n",
       "      <td>[0.8511126634068457, 0.7154417470346969, 0.6211195611519761, 0.5462184873949579]</td>\n",
       "      <td>0.851324</td>\n",
       "      <td>0.861355</td>\n",
       "      <td>10021</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.505639</td>\n",
       "      <td>0.574299</td>\n",
       "      <td>[0.8537098227696005, 0.7185935239790809, 0.6245148366094905, 0.549162730785745]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>0.574233</td>\n",
       "      <td>[0.852858856457417, 0.7174589071523767, 0.6230634682658671, 0.5472587093089663]</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.859893</td>\n",
       "      <td>10004</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.500526</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>[0.8562832213418915, 0.7219930888418237, 0.6281520511855476, 0.5527184048199685]</td>\n",
       "      <td>0.846383</td>\n",
       "      <td>0.857057</td>\n",
       "      <td>9971</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.594800</td>\n",
       "      <td>0.497747</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>[0.8548823234852279, 0.7195325542570952, 0.6250469630557295, 0.5480314960629922]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.496490</td>\n",
       "      <td>0.576910</td>\n",
       "      <td>[0.8564842653838445, 0.7223212296725329, 0.6288543494610178, 0.5530237890513041]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.575300</td>\n",
       "      <td>0.494137</td>\n",
       "      <td>0.577359</td>\n",
       "      <td>[0.8558405129232619, 0.7222222222222222, 0.6294161864194437, 0.5537095388140933]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>0.494782</td>\n",
       "      <td>0.574821</td>\n",
       "      <td>[0.855252606255012, 0.7198083778966132, 0.6265045135406219, 0.5503153669724771]</td>\n",
       "      <td>0.846878</td>\n",
       "      <td>0.857487</td>\n",
       "      <td>9976</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>0.489653</td>\n",
       "      <td>0.576563</td>\n",
       "      <td>[0.8551696526874186, 0.7210543877210543, 0.6275810286572394, 0.5512802174224002]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.495076</td>\n",
       "      <td>0.581442</td>\n",
       "      <td>[0.8582448407132839, 0.7266755733689602, 0.6355549987471811, 0.5590088799770839]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.593600</td>\n",
       "      <td>0.489830</td>\n",
       "      <td>0.581304</td>\n",
       "      <td>[0.8580444800641154, 0.7262302382542863, 0.6346780255575044, 0.5597250071612718]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.490129</td>\n",
       "      <td>0.582815</td>\n",
       "      <td>[0.8591054953870838, 0.7288230049041462, 0.6378575012543903, 0.5626792885829031]</td>\n",
       "      <td>0.846482</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>9972</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>0.485182</td>\n",
       "      <td>0.581674</td>\n",
       "      <td>[0.8590785907859079, 0.7284391386812451, 0.6371970362928544, 0.5615395662789028]</td>\n",
       "      <td>0.845590</td>\n",
       "      <td>0.856369</td>\n",
       "      <td>9963</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.598500</td>\n",
       "      <td>0.483124</td>\n",
       "      <td>0.585692</td>\n",
       "      <td>[0.8594892338507761, 0.7306622148024485, 0.6408265497808391, 0.5660701503221188]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.481823</td>\n",
       "      <td>0.585397</td>\n",
       "      <td>[0.86029706945002, 0.7313699241410084, 0.64251632345555, 0.5679207352096496]</td>\n",
       "      <td>0.845689</td>\n",
       "      <td>0.856455</td>\n",
       "      <td>9964</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.485086</td>\n",
       "      <td>0.585832</td>\n",
       "      <td>[0.860413737698333, 0.7324179504353651, 0.6433777330987686, 0.5695602184535786]</td>\n",
       "      <td>0.845094</td>\n",
       "      <td>0.855939</td>\n",
       "      <td>9958</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.483694</td>\n",
       "      <td>0.586680</td>\n",
       "      <td>[0.8606285771663822, 0.7335640138408305, 0.6444276919210956, 0.5706279637879006]</td>\n",
       "      <td>0.845194</td>\n",
       "      <td>0.856025</td>\n",
       "      <td>9959</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.474093</td>\n",
       "      <td>0.587163</td>\n",
       "      <td>[0.8587032768814511, 0.7322641719567881, 0.6438150144128337, 0.569995701389884]</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>9979</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.475508</td>\n",
       "      <td>0.585596</td>\n",
       "      <td>[0.8583016222711797, 0.7300244825283775, 0.6409967443025294, 0.5665616948182078]</td>\n",
       "      <td>0.847867</td>\n",
       "      <td>0.858346</td>\n",
       "      <td>9986</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.475017</td>\n",
       "      <td>0.584852</td>\n",
       "      <td>[0.8612955139810903, 0.7321628271080295, 0.6432888441198691, 0.569720541630654]</td>\n",
       "      <td>0.843507</td>\n",
       "      <td>0.854564</td>\n",
       "      <td>9942</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.470903</td>\n",
       "      <td>0.584967</td>\n",
       "      <td>[0.8596051708588035, 0.7304822363292126, 0.640431131720767, 0.5652672302622153]</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>9979</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.472279</td>\n",
       "      <td>0.587158</td>\n",
       "      <td>[0.8603883106485188, 0.7314279359430605, 0.642017017017017, 0.5676487414187643]</td>\n",
       "      <td>0.848461</td>\n",
       "      <td>0.858862</td>\n",
       "      <td>9992</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.469884</td>\n",
       "      <td>0.588923</td>\n",
       "      <td>[0.8603815040447418, 0.7322756019083546, 0.642705603394484, 0.567660059888778]</td>\n",
       "      <td>0.850535</td>\n",
       "      <td>0.860667</td>\n",
       "      <td>10013</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.470393</td>\n",
       "      <td>0.589911</td>\n",
       "      <td>[0.8631536766179122, 0.7354709418837675, 0.6463292407917816, 0.5721856201661415]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.471895</td>\n",
       "      <td>0.590281</td>\n",
       "      <td>[0.8634771294164748, 0.7346235124012902, 0.6456013014641472, 0.572307252181376]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.551800</td>\n",
       "      <td>0.469072</td>\n",
       "      <td>0.589191</td>\n",
       "      <td>[0.8643337009926803, 0.7350941714030982, 0.6458045904929136, 0.5717768535780869]</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>9973</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.465823</td>\n",
       "      <td>0.593436</td>\n",
       "      <td>[0.8645187112267361, 0.73815877251501, 0.649737302977233, 0.5766371175293108]</td>\n",
       "      <td>0.848658</td>\n",
       "      <td>0.859034</td>\n",
       "      <td>9994</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.467763</td>\n",
       "      <td>0.591369</td>\n",
       "      <td>[0.8624349219062876, 0.7354250111259457, 0.6478467701552328, 0.5754149971379507]</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>9988</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.462742</td>\n",
       "      <td>0.591566</td>\n",
       "      <td>[0.8630988173982762, 0.736466919135665, 0.6489095011281023, 0.5766695328174262]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.495400</td>\n",
       "      <td>0.463135</td>\n",
       "      <td>0.593399</td>\n",
       "      <td>[0.8637364837805367, 0.7380952380952381, 0.650350525788683, 0.5781339439038351]</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>9988</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.460453</td>\n",
       "      <td>0.592740</td>\n",
       "      <td>[0.8646458583433373, 0.7377723432636727, 0.6484492246123061, 0.574757004002287]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>0.459576</td>\n",
       "      <td>0.591899</td>\n",
       "      <td>[0.8637773996596937, 0.7372928484039595, 0.6478538355650106, 0.5743098269203262]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.460654</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>[0.8633223190147191, 0.7382886391454323, 0.6508075622887192, 0.5785029340203235]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.457754</td>\n",
       "      <td>0.592090</td>\n",
       "      <td>[0.8639823717948718, 0.7374220837043634, 0.6489228456913828, 0.5757445589919816]</td>\n",
       "      <td>0.847670</td>\n",
       "      <td>0.858174</td>\n",
       "      <td>9984</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.455576</td>\n",
       "      <td>0.590613</td>\n",
       "      <td>[0.8621759583625263, 0.7352908464019575, 0.6464772869478163, 0.573165498498069]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.454729</td>\n",
       "      <td>0.592033</td>\n",
       "      <td>[0.8634634634634635, 0.7369299221357063, 0.6485607008760951, 0.574964234620887]</td>\n",
       "      <td>0.848263</td>\n",
       "      <td>0.858690</td>\n",
       "      <td>9990</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.591066</td>\n",
       "      <td>[0.8653826863276156, 0.7373174266919389, 0.6488894466055967, 0.5749748887932271]</td>\n",
       "      <td>0.846185</td>\n",
       "      <td>0.856885</td>\n",
       "      <td>9969</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.454111</td>\n",
       "      <td>0.591705</td>\n",
       "      <td>[0.8647672552166934, 0.7386262265834077, 0.6495983935742972, 0.5764925373134329]</td>\n",
       "      <td>0.846086</td>\n",
       "      <td>0.856799</td>\n",
       "      <td>9968</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.562100</td>\n",
       "      <td>0.454129</td>\n",
       "      <td>0.596766</td>\n",
       "      <td>[0.8665063138905592, 0.7430385386500334, 0.6558034595136626, 0.5834049871023216]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.456443</td>\n",
       "      <td>0.594138</td>\n",
       "      <td>[0.8648053637546282, 0.738908039586345, 0.6506943575628675, 0.578006578006578]</td>\n",
       "      <td>0.848560</td>\n",
       "      <td>0.858948</td>\n",
       "      <td>9993</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.453394</td>\n",
       "      <td>0.596246</td>\n",
       "      <td>[0.8646083133493205, 0.7402309058614565, 0.6522227772227772, 0.5799086757990868]</td>\n",
       "      <td>0.850042</td>\n",
       "      <td>0.860237</td>\n",
       "      <td>10008</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.451710</td>\n",
       "      <td>0.594887</td>\n",
       "      <td>[0.8650245318914589, 0.7401802603760989, 0.6521848003004883, 0.5800772863890081]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.452667</td>\n",
       "      <td>0.596084</td>\n",
       "      <td>[0.8665931642778391, 0.7420073521220898, 0.6545067067819983, 0.5829152931059195]</td>\n",
       "      <td>0.846977</td>\n",
       "      <td>0.857573</td>\n",
       "      <td>9977</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.452807</td>\n",
       "      <td>0.594702</td>\n",
       "      <td>[0.8660392470965158, 0.7403204272363151, 0.6517275913870806, 0.5787063537492845]</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>9988</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.537400</td>\n",
       "      <td>0.448922</td>\n",
       "      <td>0.595976</td>\n",
       "      <td>[0.8643457382953181, 0.7402178746109382, 0.6534517258629314, 0.5811892510005717]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.505700</td>\n",
       "      <td>0.448492</td>\n",
       "      <td>0.592678</td>\n",
       "      <td>[0.8650140505820956, 0.7396251673360107, 0.6515570065293822, 0.5786904078116025]</td>\n",
       "      <td>0.845689</td>\n",
       "      <td>0.856455</td>\n",
       "      <td>9964</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.521200</td>\n",
       "      <td>0.446830</td>\n",
       "      <td>0.594157</td>\n",
       "      <td>[0.8656716417910447, 0.739953245018368, 0.6516347237880497, 0.5785479020478305]</td>\n",
       "      <td>0.847571</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>9983</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.447119</td>\n",
       "      <td>0.591339</td>\n",
       "      <td>[0.863431715857929, 0.7352973874374653, 0.6469043151969981, 0.5736954967834167]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.446594</td>\n",
       "      <td>0.594038</td>\n",
       "      <td>[0.8652787508757882, 0.7388499610721833, 0.650857214366162, 0.5777428121870977]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.443628</td>\n",
       "      <td>0.594975</td>\n",
       "      <td>[0.8651674162918541, 0.7391449194891727, 0.6507183010618364, 0.577587437544611]</td>\n",
       "      <td>0.849745</td>\n",
       "      <td>0.859979</td>\n",
       "      <td>10005</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.444265</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>[0.8690858059342422, 0.7445409982174688, 0.6574724172517553, 0.5854357798165137]</td>\n",
       "      <td>0.846878</td>\n",
       "      <td>0.857487</td>\n",
       "      <td>9976</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.442155</td>\n",
       "      <td>0.597467</td>\n",
       "      <td>[0.8677487225728885, 0.7439037969045763, 0.6560581380779351, 0.5835840137516115]</td>\n",
       "      <td>0.847373</td>\n",
       "      <td>0.857916</td>\n",
       "      <td>9981</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.597858</td>\n",
       "      <td>[0.867046818727491, 0.7426634059582037, 0.6555777888944472, 0.582904516866781]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.441935</td>\n",
       "      <td>0.598786</td>\n",
       "      <td>[0.8701324769169009, 0.7459839357429718, 0.6595931692616775, 0.5870189546237794]</td>\n",
       "      <td>0.845689</td>\n",
       "      <td>0.856455</td>\n",
       "      <td>9964</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.443357</td>\n",
       "      <td>0.599037</td>\n",
       "      <td>[0.8694605975536395, 0.7451526632493871, 0.6585151743165287, 0.5873243475767135]</td>\n",
       "      <td>0.846680</td>\n",
       "      <td>0.857315</td>\n",
       "      <td>9974</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.442544</td>\n",
       "      <td>0.598042</td>\n",
       "      <td>[0.8680437180387045, 0.7440098071993759, 0.6577198043396463, 0.5862612935608776]</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>9973</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.440794</td>\n",
       "      <td>0.599550</td>\n",
       "      <td>[0.8693951248871502, 0.7462370386888171, 0.6598067511607479, 0.5887501793657627]</td>\n",
       "      <td>0.846185</td>\n",
       "      <td>0.856885</td>\n",
       "      <td>9969</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.439092</td>\n",
       "      <td>0.600912</td>\n",
       "      <td>[0.8681813632269042, 0.7465243020798576, 0.6599924915529971, 0.5884708911457588]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.439135</td>\n",
       "      <td>0.601523</td>\n",
       "      <td>[0.8688770910547932, 0.7475230991873539, 0.6615307528498059, 0.5904339109265359]</td>\n",
       "      <td>0.847571</td>\n",
       "      <td>0.858088</td>\n",
       "      <td>9983</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.436114</td>\n",
       "      <td>0.600660</td>\n",
       "      <td>[0.8663400979706088, 0.7438631567255359, 0.6591278270648507, 0.5883192917321148]</td>\n",
       "      <td>0.849548</td>\n",
       "      <td>0.859807</td>\n",
       "      <td>10003</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.603627</td>\n",
       "      <td>[0.8693346673336668, 0.7487493051695386, 0.6634146341463415, 0.5924231593995711]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.439010</td>\n",
       "      <td>0.600766</td>\n",
       "      <td>[0.8699488619272034, 0.7469073888331662, 0.661357080145491, 0.5901333715760791]</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>9973</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>0.436842</td>\n",
       "      <td>0.600612</td>\n",
       "      <td>[0.8695608582314016, 0.7471584577668821, 0.6610233258088789, 0.5896185833094351]</td>\n",
       "      <td>0.846680</td>\n",
       "      <td>0.857315</td>\n",
       "      <td>9974</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.436534</td>\n",
       "      <td>0.601668</td>\n",
       "      <td>[0.8697091273821465, 0.7482720178372352, 0.6628607277289836, 0.5922525107604018]</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>0.856971</td>\n",
       "      <td>9970</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>0.434383</td>\n",
       "      <td>0.602008</td>\n",
       "      <td>[0.869195356285028, 0.7472197508896797, 0.6612862862862863, 0.5901029748283753]</td>\n",
       "      <td>0.848461</td>\n",
       "      <td>0.858862</td>\n",
       "      <td>9992</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.434432</td>\n",
       "      <td>0.601111</td>\n",
       "      <td>[0.865934065934066, 0.744173140954495, 0.6591760299625468, 0.5881597717546362]</td>\n",
       "      <td>0.850239</td>\n",
       "      <td>0.860409</td>\n",
       "      <td>10010</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>0.436918</td>\n",
       "      <td>0.603249</td>\n",
       "      <td>[0.8703184458241539, 0.748831515691075, 0.6636614074630603, 0.5924706555969081]</td>\n",
       "      <td>0.847867</td>\n",
       "      <td>0.858346</td>\n",
       "      <td>9986</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.433498</td>\n",
       "      <td>0.601117</td>\n",
       "      <td>[0.8670734146829366, 0.7449433207379418, 0.6600400100025007, 0.5893112317805087]</td>\n",
       "      <td>0.849054</td>\n",
       "      <td>0.859378</td>\n",
       "      <td>9998</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.433308</td>\n",
       "      <td>0.602853</td>\n",
       "      <td>[0.8691036554832249, 0.7488035614913745, 0.663243581715717, 0.5924123120973515]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.430156</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>[0.8691167634688564, 0.7470509681727131, 0.6611570247933884, 0.5898940738620098]</td>\n",
       "      <td>0.847867</td>\n",
       "      <td>0.858346</td>\n",
       "      <td>9986</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.432415</td>\n",
       "      <td>0.601640</td>\n",
       "      <td>[0.8673203922353412, 0.7461641094062709, 0.6611208406304728, 0.590363168430083]</td>\n",
       "      <td>0.848658</td>\n",
       "      <td>0.859034</td>\n",
       "      <td>9994</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.431649</td>\n",
       "      <td>0.603994</td>\n",
       "      <td>[0.8693215929557735, 0.7487213698020903, 0.6639979984988742, 0.5936517014583929]</td>\n",
       "      <td>0.848658</td>\n",
       "      <td>0.859034</td>\n",
       "      <td>9994</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.431194</td>\n",
       "      <td>0.603495</td>\n",
       "      <td>[0.8712295821224572, 0.7493039313954784, 0.664494297531019, 0.5936380570282276]</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>9979</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.430655</td>\n",
       "      <td>0.603883</td>\n",
       "      <td>[0.8698609582874862, 0.7488051572746471, 0.6634988120545204, 0.5923967414606259]</td>\n",
       "      <td>0.848955</td>\n",
       "      <td>0.859292</td>\n",
       "      <td>9997</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.429037</td>\n",
       "      <td>0.605507</td>\n",
       "      <td>[0.8714485794317727, 0.7507781236104936, 0.6653326663331666, 0.5947684391080618]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.429323</td>\n",
       "      <td>0.604840</td>\n",
       "      <td>[0.8711, 0.7501111111111111, 0.664, 0.593]</td>\n",
       "      <td>0.849251</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>10000</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.427774</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>[0.8700129987001299, 0.7470281079880013, 0.6606674165729284, 0.589487216111984]</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>10001</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>0.426878</td>\n",
       "      <td>0.602084</td>\n",
       "      <td>[0.8695086560592414, 0.7471366618481041, 0.6610784436381834, 0.5901615901615902]</td>\n",
       "      <td>0.848560</td>\n",
       "      <td>0.858948</td>\n",
       "      <td>9993</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.428542</td>\n",
       "      <td>0.603031</td>\n",
       "      <td>[0.8704870487048705, 0.7479719968885432, 0.6618327290911364, 0.5902271753107586]</td>\n",
       "      <td>0.849153</td>\n",
       "      <td>0.859464</td>\n",
       "      <td>9999</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.426982</td>\n",
       "      <td>0.603676</td>\n",
       "      <td>[0.8710355177588794, 0.7486381322957198, 0.6629143214509068, 0.5919942816297356]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.427217</td>\n",
       "      <td>0.603924</td>\n",
       "      <td>[0.871006509764647, 0.7495826377295493, 0.6643706950532248, 0.5937007874015748]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.427334</td>\n",
       "      <td>0.604053</td>\n",
       "      <td>[0.8712424849699398, 0.750445434298441, 0.6649122807017543, 0.594269340974212]</td>\n",
       "      <td>0.847274</td>\n",
       "      <td>0.857830</td>\n",
       "      <td>9980</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.427549</td>\n",
       "      <td>0.604591</td>\n",
       "      <td>[0.872208312468703, 0.7504730105731775, 0.6651221039448967, 0.5941302791696492]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.426655</td>\n",
       "      <td>0.603124</td>\n",
       "      <td>[0.8699609882964889, 0.7479159719906635, 0.6619982493435038, 0.5913963127054452]</td>\n",
       "      <td>0.848955</td>\n",
       "      <td>0.859292</td>\n",
       "      <td>9997</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.427483</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>[0.871132472213878, 0.7497496383665294, 0.6639539251283335, 0.5928152282810935]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>0.426797</td>\n",
       "      <td>0.604493</td>\n",
       "      <td>[0.8713684632338209, 0.7506123357826765, 0.6653720871961915, 0.5948152391864795]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.426501</td>\n",
       "      <td>0.602448</td>\n",
       "      <td>[0.8720626631853786, 0.7506139763340032, 0.6648655441065594, 0.5934176487496406]</td>\n",
       "      <td>0.845094</td>\n",
       "      <td>0.855939</td>\n",
       "      <td>9958</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.424364</td>\n",
       "      <td>0.604556</td>\n",
       "      <td>[0.8707319515369981, 0.7503059975520195, 0.6653311631401027, 0.5943895806497782]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.497200</td>\n",
       "      <td>0.424940</td>\n",
       "      <td>0.604254</td>\n",
       "      <td>[0.871332732552318, 0.7498609102036274, 0.6645799424064104, 0.5938170888793474]</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>9987</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>0.424217</td>\n",
       "      <td>0.604617</td>\n",
       "      <td>[0.8721291746063584, 0.7515327165310445, 0.6664157571195584, 0.5961841916511261]</td>\n",
       "      <td>0.846383</td>\n",
       "      <td>0.857057</td>\n",
       "      <td>9971</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.424683</td>\n",
       "      <td>0.602962</td>\n",
       "      <td>[0.8720673751754562, 0.7497214174281257, 0.6636568848758465, 0.5927731574419272]</td>\n",
       "      <td>0.846680</td>\n",
       "      <td>0.857315</td>\n",
       "      <td>9974</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.426990</td>\n",
       "      <td>0.604654</td>\n",
       "      <td>[0.872491974317817, 0.7516726137377342, 0.6666666666666666, 0.5965843857634903]</td>\n",
       "      <td>0.846086</td>\n",
       "      <td>0.856799</td>\n",
       "      <td>9968</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.480200</td>\n",
       "      <td>0.424245</td>\n",
       "      <td>0.604660</td>\n",
       "      <td>[0.8718565274020639, 0.7508072597706269, 0.6654554567096855, 0.5951869359690589]</td>\n",
       "      <td>0.847373</td>\n",
       "      <td>0.857916</td>\n",
       "      <td>9981</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>0.423241</td>\n",
       "      <td>0.603518</td>\n",
       "      <td>[0.8713684632338209, 0.749276330438655, 0.663868704585317, 0.5933829848181037]</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>0.858002</td>\n",
       "      <td>9982</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.423646</td>\n",
       "      <td>0.603934</td>\n",
       "      <td>[0.8703481392557023, 0.7488883948421521, 0.6633316658329165, 0.5926243567753001]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.423354</td>\n",
       "      <td>0.607158</td>\n",
       "      <td>[0.8731208659049909, 0.7530630429939853, 0.6693406868889445, 0.5997420464316423]</td>\n",
       "      <td>0.847076</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>9978</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.421917</td>\n",
       "      <td>0.607694</td>\n",
       "      <td>[0.8737105658487732, 0.7539232053422371, 0.6691296180338134, 0.5989978525411597]</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.858260</td>\n",
       "      <td>9985</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.421817</td>\n",
       "      <td>0.606394</td>\n",
       "      <td>[0.8743481748896912, 0.7540124832813196, 0.6682137481184145, 0.597819850831899]</td>\n",
       "      <td>0.846482</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>9972</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.420358</td>\n",
       "      <td>0.607818</td>\n",
       "      <td>[0.8739983974358975, 0.7541184327693677, 0.6693386773547094, 0.5992268041237113]</td>\n",
       "      <td>0.847670</td>\n",
       "      <td>0.858174</td>\n",
       "      <td>9984</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>0.419253</td>\n",
       "      <td>0.606723</td>\n",
       "      <td>[0.8734342118448742, 0.753313286557523, 0.6685048251660609, 0.5980799541481587]</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.857745</td>\n",
       "      <td>9979</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.420076</td>\n",
       "      <td>0.608559</td>\n",
       "      <td>[0.8741609057208697, 0.7548157220799465, 0.670592657561709, 0.6012032660077353]</td>\n",
       "      <td>0.847373</td>\n",
       "      <td>0.857916</td>\n",
       "      <td>9981</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.418435</td>\n",
       "      <td>0.606995</td>\n",
       "      <td>[0.8727218105347486, 0.7527264633874916, 0.6682945154019534, 0.5983395362152877]</td>\n",
       "      <td>0.847867</td>\n",
       "      <td>0.858346</td>\n",
       "      <td>9986</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>0.418925</td>\n",
       "      <td>0.606096</td>\n",
       "      <td>[0.8716358179089545, 0.7509727626459144, 0.666541588492808, 0.5959971408148678]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.419499</td>\n",
       "      <td>0.608403</td>\n",
       "      <td>[0.8733113179225458, 0.7539197153341488, 0.6697109971224822, 0.5993135993135993]</td>\n",
       "      <td>0.848560</td>\n",
       "      <td>0.858948</td>\n",
       "      <td>9993</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.419030</td>\n",
       "      <td>0.607310</td>\n",
       "      <td>[0.8713543747502996, 0.7512205947625389, 0.6666250624063904, 0.5959783228750714]</td>\n",
       "      <td>0.850436</td>\n",
       "      <td>0.860581</td>\n",
       "      <td>10012</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.606851</td>\n",
       "      <td>[0.872336168084042, 0.7520844913841023, 0.6675422138836773, 0.5967119370979271]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.418645</td>\n",
       "      <td>0.608186</td>\n",
       "      <td>[0.8724127587241276, 0.7532496389290079, 0.6687914010748657, 0.5982002571061277]</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>10001</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.420502</td>\n",
       "      <td>0.605803</td>\n",
       "      <td>[0.8724852367130418, 0.7517517517517518, 0.6663746715054436, 0.5949077385209555]</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>9991</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.419633</td>\n",
       "      <td>0.605686</td>\n",
       "      <td>[0.8712970376301041, 0.7512233096085409, 0.6665415415415415, 0.5952517162471396]</td>\n",
       "      <td>0.848461</td>\n",
       "      <td>0.858862</td>\n",
       "      <td>9992</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>0.607386</td>\n",
       "      <td>[0.8709387183844847, 0.7517494168610463, 0.6676246407597151, 0.5977438240753963]</td>\n",
       "      <td>0.849548</td>\n",
       "      <td>0.859807</td>\n",
       "      <td>10003</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.418054</td>\n",
       "      <td>0.606604</td>\n",
       "      <td>[0.8708903767362846, 0.7507494171200177, 0.6661671037841889, 0.5956900242614528]</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.860151</td>\n",
       "      <td>10007</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>0.418247</td>\n",
       "      <td>0.607478</td>\n",
       "      <td>[0.8710063897763578, 0.7514418811002662, 0.6664171656686627, 0.5957810718358039]</td>\n",
       "      <td>0.850831</td>\n",
       "      <td>0.860925</td>\n",
       "      <td>10016</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.418743</td>\n",
       "      <td>0.608622</td>\n",
       "      <td>[0.8714799281006591, 0.752607055691147, 0.6680808584976291, 0.5980895352152837]</td>\n",
       "      <td>0.850634</td>\n",
       "      <td>0.860753</td>\n",
       "      <td>10014</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.418569</td>\n",
       "      <td>0.608414</td>\n",
       "      <td>[0.8724510195921631, 0.7532207907596624, 0.6686656671664168, 0.5983723586521987]</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.859893</td>\n",
       "      <td>10004</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.419004</td>\n",
       "      <td>0.606284</td>\n",
       "      <td>[0.8716871687168717, 0.7513057006334037, 0.6663332916614577, 0.5955136448064009]</td>\n",
       "      <td>0.849153</td>\n",
       "      <td>0.859464</td>\n",
       "      <td>9999</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.419072</td>\n",
       "      <td>0.608111</td>\n",
       "      <td>[0.8735367683841921, 0.7530850472484714, 0.6690431519699812, 0.5987133666904932]</td>\n",
       "      <td>0.848757</td>\n",
       "      <td>0.859120</td>\n",
       "      <td>9995</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.418665</td>\n",
       "      <td>0.608025</td>\n",
       "      <td>[0.8734493797519007, 0.753223654957759, 0.6687093546773387, 0.5983419096626644]</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.859206</td>\n",
       "      <td>9996</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.418072</td>\n",
       "      <td>0.607992</td>\n",
       "      <td>[0.872189467372839, 0.7521927389807928, 0.6679155738728613, 0.5975453118310261]</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.860151</td>\n",
       "      <td>10007</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.418161</td>\n",
       "      <td>0.609428</td>\n",
       "      <td>[0.8726273726273727, 0.753496115427303, 0.6695380774032459, 0.5995720399429386]</td>\n",
       "      <td>0.850239</td>\n",
       "      <td>0.860409</td>\n",
       "      <td>10010</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.418262</td>\n",
       "      <td>0.608618</td>\n",
       "      <td>[0.8723765740555667, 0.7524983344437042, 0.6688733449912565, 0.59905795032829]</td>\n",
       "      <td>0.849844</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>10006</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.417622</td>\n",
       "      <td>0.608078</td>\n",
       "      <td>[0.8724020783373302, 0.7521092362344582, 0.6678321678321678, 0.5976027397260274]</td>\n",
       "      <td>0.850042</td>\n",
       "      <td>0.860237</td>\n",
       "      <td>10008</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.416936</td>\n",
       "      <td>0.608244</td>\n",
       "      <td>[0.8715027977617905, 0.7518872113676732, 0.6683316683316683, 0.598601598173516]</td>\n",
       "      <td>0.850042</td>\n",
       "      <td>0.860237</td>\n",
       "      <td>10008</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.416327</td>\n",
       "      <td>0.607740</td>\n",
       "      <td>[0.8710418539606433, 0.7513039618244368, 0.6674572462863563, 0.59734702610184]</td>\n",
       "      <td>0.850338</td>\n",
       "      <td>0.860495</td>\n",
       "      <td>10011</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.415779</td>\n",
       "      <td>0.607528</td>\n",
       "      <td>[0.8718256348730254, 0.7518329260164408, 0.6677080729817546, 0.5978291916595259]</td>\n",
       "      <td>0.849449</td>\n",
       "      <td>0.859722</td>\n",
       "      <td>10002</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>0.415573</td>\n",
       "      <td>0.607139</td>\n",
       "      <td>[0.8715642178910544, 0.751471404775125, 0.6668332292317302, 0.596716630977873]</td>\n",
       "      <td>0.849745</td>\n",
       "      <td>0.859979</td>\n",
       "      <td>10005</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.608081</td>\n",
       "      <td>[0.8726745349069814, 0.7528339631029117, 0.6686671667916979, 0.5988853958273792]</td>\n",
       "      <td>0.849054</td>\n",
       "      <td>0.859378</td>\n",
       "      <td>9998</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.414876</td>\n",
       "      <td>0.608009</td>\n",
       "      <td>[0.8726127387261274, 0.7525830463281857, 0.6682914635670542, 0.5983430938437366]</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>10001</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.415177</td>\n",
       "      <td>0.607746</td>\n",
       "      <td>[0.8723127687231277, 0.7523608487945784, 0.6680414948131483, 0.5979145836309099]</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>10001</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.415160</td>\n",
       "      <td>0.608438</td>\n",
       "      <td>[0.8725637181409296, 0.7529150471960022, 0.6685821361648969, 0.5984296930763741]</td>\n",
       "      <td>0.849745</td>\n",
       "      <td>0.859979</td>\n",
       "      <td>10005</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.415415</td>\n",
       "      <td>0.608491</td>\n",
       "      <td>[0.8726509396241503, 0.7528876055086628, 0.6686656671664168, 0.5988006853226727]</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.859893</td>\n",
       "      <td>10004</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.415146</td>\n",
       "      <td>0.607223</td>\n",
       "      <td>[0.8716769938037178, 0.7514990006662225, 0.6668748438670997, 0.5966314587496432]</td>\n",
       "      <td>0.849844</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>10006</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.414751</td>\n",
       "      <td>0.607105</td>\n",
       "      <td>[0.8715898870790447, 0.7513045409126236, 0.6665417759460472, 0.5964035964035964]</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.860151</td>\n",
       "      <td>10007</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.414848</td>\n",
       "      <td>0.606794</td>\n",
       "      <td>[0.871664167916042, 0.7511382565241532, 0.6663335415365397, 0.5960028551034975]</td>\n",
       "      <td>0.849745</td>\n",
       "      <td>0.859979</td>\n",
       "      <td>10005</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.414848</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>[0.8721127887211279, 0.7514720586601489, 0.6669166354205724, 0.5966290529924296]</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>10001</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.414776</td>\n",
       "      <td>0.606506</td>\n",
       "      <td>[0.8721, 0.7512222222222222, 0.666375, 0.5958571428571429]</td>\n",
       "      <td>0.849251</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>10000</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.414781</td>\n",
       "      <td>0.607155</td>\n",
       "      <td>[0.871864067966017, 0.751471404775125, 0.6668332292317302, 0.5965738758029978]</td>\n",
       "      <td>0.849745</td>\n",
       "      <td>0.859979</td>\n",
       "      <td>10005</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.414550</td>\n",
       "      <td>0.607276</td>\n",
       "      <td>[0.8719896072749076, 0.7515265904296659, 0.6667915573872861, 0.5964035964035964]</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.860151</td>\n",
       "      <td>10007</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.414751</td>\n",
       "      <td>0.606893</td>\n",
       "      <td>[0.8717897471769761, 0.7510824913955813, 0.6662919945048082, 0.5958327386898815]</td>\n",
       "      <td>0.849943</td>\n",
       "      <td>0.860151</td>\n",
       "      <td>10007</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.414868</td>\n",
       "      <td>0.606920</td>\n",
       "      <td>[0.8717769338396962, 0.7510548523206751, 0.6665001249063203, 0.5960605195546674]</td>\n",
       "      <td>0.849844</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>10006</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.414727</td>\n",
       "      <td>0.607138</td>\n",
       "      <td>[0.8720511795281887, 0.7515548645046646, 0.6669165417291354, 0.596516276413478]</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.859893</td>\n",
       "      <td>10004</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.414734</td>\n",
       "      <td>0.606921</td>\n",
       "      <td>[0.8715770537677393, 0.7510548523206751, 0.6665001249063203, 0.5962032543534114]</td>\n",
       "      <td>0.849844</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>10006</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.414718</td>\n",
       "      <td>0.606961</td>\n",
       "      <td>[0.8716769938037178, 0.751165889407062, 0.6665001249063203, 0.5962032543534114]</td>\n",
       "      <td>0.849844</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>10006</td>\n",
       "      <td>11634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.5535853685736589, 0.3063888540228442, 0.19865078225922206, 0.10631895687061184]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.6930783242258652, 0.47613150191398335, 0.35705225773718924, 0.24803950043566658]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.7549382716049383, 0.5360091743119266, 0.4134403729120808, 0.3062741599762117]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.7730445493738453, 0.5666895447266072, 0.44894797986317286, 0.3474011550422035]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.7899117020196894, 0.5882751609623856, 0.47021384928716903, 0.36973319725907566]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.7991504854368932, 0.6073357335733574, 0.49359868170870835, 0.3969521044992743]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8077895801719778, 0.6227349465391109, 0.5101445599797109, 0.4167271671264702]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8133400707427993, 0.6345137717818999, 0.5213426219126029, 0.4293793503480278]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.81591737545565, 0.6411671924290221, 0.5298374809547994, 0.4375454413261597]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8192136197811106, 0.6472710870545783, 0.5374936451448907, 0.44577085456398313]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8186957399555824, 0.6472041320458118, 0.5365545155578042, 0.44592442449688724]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8221774193548387, 0.6552690582959642, 0.5467171717171717, 0.4578818089871406]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8280998389694042, 0.661705461056401, 0.5525453629032258, 0.46352364475201846]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8268167655040708, 0.6623086378366297, 0.5544093596678827, 0.4665419484817959]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8318423962207258, 0.6696837635489998, 0.5630896968172097, 0.47704705713052237]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8334172625642059, 0.6753275842759547, 0.56930256022197, 0.4811661134362823]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8360688752391502, 0.6785354383607659, 0.5735720590089523, 0.4873755590823835]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8313760915386932, 0.6749972107553275, 0.5698857214617606, 0.4835559385322418]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8363508348420841, 0.6788190561395661, 0.5754218081087887, 0.49121290694324404]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8380684670213834, 0.6848566008258007, 0.5820876774274588, 0.49777330843269646]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8367428800641797, 0.6851315202853322, 0.5835423983943804, 0.4995697074010327]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8368104312938817, 0.6875139353400223, 0.5870765370138018, 0.5045911047345768]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8369805259987955, 0.6870118277170274, 0.5864104496357699, 0.5040218328066648]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8385447985568251, 0.6913566495878815, 0.5926297317623465, 0.5103181427343079]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8398476648626979, 0.6925818667854756, 0.5936324893457007, 0.5123244482659788]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8394665597112203, 0.6935250195029533, 0.5943810359964882, 0.5134088627563459]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8378756114605171, 0.6911389597427082, 0.590994137457902, 0.5096194955109021]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8377784437119169, 0.6929308622794362, 0.594931968543253, 0.5154756810726002]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8418846077061289, 0.6979374584165003, 0.5991519082065353, 0.518666286691365]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8416942024632021, 0.6973406030933571, 0.5994741454864154, 0.5193931587233434]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8436498397435898, 0.70013357079252, 0.6019539078156313, 0.5216208476517755]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8456907795725895, 0.7035797925727668, 0.605623195682189, 0.5261949189034018]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8433530059017705, 0.7025675225075025, 0.607102663498812, 0.5279405459482636]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8445310937812438, 0.7027327260608753, 0.6069732566858286, 0.5274207369323051]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8430331563658219, 0.7021039741734387, 0.6057872980082676, 0.5268509236717743]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8468865938032688, 0.7071213640922769, 0.6119402985074627, 0.5334863043166499]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8446398241582576, 0.703962703962704, 0.6080659258334374, 0.5297474675417321]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8471694338867773, 0.7086019115358969, 0.6122780695173794, 0.5332190312901843]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8480367669097812, 0.7086247086247086, 0.6123111499562992, 0.534170352404052]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.848381941669996, 0.7102751886373724, 0.61445332001997, 0.5370792926411866]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8488488488488488, 0.7100111234705228, 0.6141426783479349, 0.5374821173104435]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8524952304448238, 0.7154816385757339, 0.6210579218494786, 0.5453369737031183]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8509490509490509, 0.7155382907880133, 0.6217228464419475, 0.5456490727532097]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8510169321711252, 0.7135062910589022, 0.6179676732239068, 0.5406102277610657]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8523261630815407, 0.7175097276264591, 0.6220137585991244, 0.5461043602573267]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8511126634068457, 0.7154417470346969, 0.6211195611519761, 0.5462184873949579]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8537098227696005, 0.7185935239790809, 0.6245148366094905, 0.549162730785745]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.852858856457417, 0.7174589071523767, 0.6230634682658671, 0.5472587093089663]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8562832213418915, 0.7219930888418237, 0.6281520511855476, 0.5527184048199685]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8548823234852279, 0.7195325542570952, 0.6250469630557295, 0.5480314960629922]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8564842653838445, 0.7223212296725329, 0.6288543494610178, 0.5530237890513041]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8558405129232619, 0.7222222222222222, 0.6294161864194437, 0.5537095388140933]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.855252606255012, 0.7198083778966132, 0.6265045135406219, 0.5503153669724771]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8551696526874186, 0.7210543877210543, 0.6275810286572394, 0.5512802174224002]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8582448407132839, 0.7266755733689602, 0.6355549987471811, 0.5590088799770839]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8580444800641154, 0.7262302382542863, 0.6346780255575044, 0.5597250071612718]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8591054953870838, 0.7288230049041462, 0.6378575012543903, 0.5626792885829031]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8590785907859079, 0.7284391386812451, 0.6371970362928544, 0.5615395662789028]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8594892338507761, 0.7306622148024485, 0.6408265497808391, 0.5660701503221188]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.86029706945002, 0.7313699241410084, 0.64251632345555, 0.5679207352096496]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.860413737698333, 0.7324179504353651, 0.6433777330987686, 0.5695602184535786]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8606285771663822, 0.7335640138408305, 0.6444276919210956, 0.5706279637879006]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8587032768814511, 0.7322641719567881, 0.6438150144128337, 0.569995701389884]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8583016222711797, 0.7300244825283775, 0.6409967443025294, 0.5665616948182078]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8612955139810903, 0.7321628271080295, 0.6432888441198691, 0.569720541630654]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8596051708588035, 0.7304822363292126, 0.640431131720767, 0.5652672302622153]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8603883106485188, 0.7314279359430605, 0.642017017017017, 0.5676487414187643]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8603815040447418, 0.7322756019083546, 0.642705603394484, 0.567660059888778]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8631536766179122, 0.7354709418837675, 0.6463292407917816, 0.5721856201661415]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8634771294164748, 0.7346235124012902, 0.6456013014641472, 0.572307252181376]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8643337009926803, 0.7350941714030982, 0.6458045904929136, 0.5717768535780869]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8645187112267361, 0.73815877251501, 0.649737302977233, 0.5766371175293108]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8624349219062876, 0.7354250111259457, 0.6478467701552328, 0.5754149971379507]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8630988173982762, 0.736466919135665, 0.6489095011281023, 0.5766695328174262]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8637364837805367, 0.7380952380952381, 0.650350525788683, 0.5781339439038351]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8646458583433373, 0.7377723432636727, 0.6484492246123061, 0.574757004002287]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8637773996596937, 0.7372928484039595, 0.6478538355650106, 0.5743098269203262]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8633223190147191, 0.7382886391454323, 0.6508075622887192, 0.5785029340203235]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8639823717948718, 0.7374220837043634, 0.6489228456913828, 0.5757445589919816]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8621759583625263, 0.7352908464019575, 0.6464772869478163, 0.573165498498069]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8634634634634635, 0.7369299221357063, 0.6485607008760951, 0.574964234620887]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8653826863276156, 0.7373174266919389, 0.6488894466055967, 0.5749748887932271]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8647672552166934, 0.7386262265834077, 0.6495983935742972, 0.5764925373134329]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8665063138905592, 0.7430385386500334, 0.6558034595136626, 0.5834049871023216]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8648053637546282, 0.738908039586345, 0.6506943575628675, 0.578006578006578]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8646083133493205, 0.7402309058614565, 0.6522227772227772, 0.5799086757990868]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8650245318914589, 0.7401802603760989, 0.6521848003004883, 0.5800772863890081]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8665931642778391, 0.7420073521220898, 0.6545067067819983, 0.5829152931059195]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8660392470965158, 0.7403204272363151, 0.6517275913870806, 0.5787063537492845]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8643457382953181, 0.7402178746109382, 0.6534517258629314, 0.5811892510005717]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8650140505820956, 0.7396251673360107, 0.6515570065293822, 0.5786904078116025]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8656716417910447, 0.739953245018368, 0.6516347237880497, 0.5785479020478305]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.863431715857929, 0.7352973874374653, 0.6469043151969981, 0.5736954967834167]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8652787508757882, 0.7388499610721833, 0.650857214366162, 0.5777428121870977]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8651674162918541, 0.7391449194891727, 0.6507183010618364, 0.577587437544611]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8690858059342422, 0.7445409982174688, 0.6574724172517553, 0.5854357798165137]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8677487225728885, 0.7439037969045763, 0.6560581380779351, 0.5835840137516115]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.867046818727491, 0.7426634059582037, 0.6555777888944472, 0.582904516866781]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8701324769169009, 0.7459839357429718, 0.6595931692616775, 0.5870189546237794]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8694605975536395, 0.7451526632493871, 0.6585151743165287, 0.5873243475767135]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8680437180387045, 0.7440098071993759, 0.6577198043396463, 0.5862612935608776]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8693951248871502, 0.7462370386888171, 0.6598067511607479, 0.5887501793657627]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8681813632269042, 0.7465243020798576, 0.6599924915529971, 0.5884708911457588]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8688770910547932, 0.7475230991873539, 0.6615307528498059, 0.5904339109265359]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8663400979706088, 0.7438631567255359, 0.6591278270648507, 0.5883192917321148]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8693346673336668, 0.7487493051695386, 0.6634146341463415, 0.5924231593995711]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8699488619272034, 0.7469073888331662, 0.661357080145491, 0.5901333715760791]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8695608582314016, 0.7471584577668821, 0.6610233258088789, 0.5896185833094351]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8697091273821465, 0.7482720178372352, 0.6628607277289836, 0.5922525107604018]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.869195356285028, 0.7472197508896797, 0.6612862862862863, 0.5901029748283753]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.865934065934066, 0.744173140954495, 0.6591760299625468, 0.5881597717546362]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8703184458241539, 0.748831515691075, 0.6636614074630603, 0.5924706555969081]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8670734146829366, 0.7449433207379418, 0.6600400100025007, 0.5893112317805087]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8691036554832249, 0.7488035614913745, 0.663243581715717, 0.5924123120973515]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8691167634688564, 0.7470509681727131, 0.6611570247933884, 0.5898940738620098]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8673203922353412, 0.7461641094062709, 0.6611208406304728, 0.590363168430083]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8693215929557735, 0.7487213698020903, 0.6639979984988742, 0.5936517014583929]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8712295821224572, 0.7493039313954784, 0.664494297531019, 0.5936380570282276]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8698609582874862, 0.7488051572746471, 0.6634988120545204, 0.5923967414606259]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8714485794317727, 0.7507781236104936, 0.6653326663331666, 0.5947684391080618]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.8711, 0.7501111111111111, 0.664, 0.593]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8700129987001299, 0.7470281079880013, 0.6606674165729284, 0.589487216111984]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8695086560592414, 0.7471366618481041, 0.6610784436381834, 0.5901615901615902]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8704870487048705, 0.7479719968885432, 0.6618327290911364, 0.5902271753107586]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8710355177588794, 0.7486381322957198, 0.6629143214509068, 0.5919942816297356]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.871006509764647, 0.7495826377295493, 0.6643706950532248, 0.5937007874015748]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8712424849699398, 0.750445434298441, 0.6649122807017543, 0.594269340974212]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.872208312468703, 0.7504730105731775, 0.6651221039448967, 0.5941302791696492]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8699609882964889, 0.7479159719906635, 0.6619982493435038, 0.5913963127054452]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.871132472213878, 0.7497496383665294, 0.6639539251283335, 0.5928152282810935]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8713684632338209, 0.7506123357826765, 0.6653720871961915, 0.5948152391864795]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8720626631853786, 0.7506139763340032, 0.6648655441065594, 0.5934176487496406]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8707319515369981, 0.7503059975520195, 0.6653311631401027, 0.5943895806497782]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.871332732552318, 0.7498609102036274, 0.6645799424064104, 0.5938170888793474]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8721291746063584, 0.7515327165310445, 0.6664157571195584, 0.5961841916511261]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8720673751754562, 0.7497214174281257, 0.6636568848758465, 0.5927731574419272]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.872491974317817, 0.7516726137377342, 0.6666666666666666, 0.5965843857634903]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8718565274020639, 0.7508072597706269, 0.6654554567096855, 0.5951869359690589]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8713684632338209, 0.749276330438655, 0.663868704585317, 0.5933829848181037]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8703481392557023, 0.7488883948421521, 0.6633316658329165, 0.5926243567753001]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8731208659049909, 0.7530630429939853, 0.6693406868889445, 0.5997420464316423]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8737105658487732, 0.7539232053422371, 0.6691296180338134, 0.5989978525411597]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8743481748896912, 0.7540124832813196, 0.6682137481184145, 0.597819850831899]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8739983974358975, 0.7541184327693677, 0.6693386773547094, 0.5992268041237113]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8734342118448742, 0.753313286557523, 0.6685048251660609, 0.5980799541481587]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8741609057208697, 0.7548157220799465, 0.670592657561709, 0.6012032660077353]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8727218105347486, 0.7527264633874916, 0.6682945154019534, 0.5983395362152877]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8716358179089545, 0.7509727626459144, 0.666541588492808, 0.5959971408148678]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8733113179225458, 0.7539197153341488, 0.6697109971224822, 0.5993135993135993]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8713543747502996, 0.7512205947625389, 0.6666250624063904, 0.5959783228750714]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.872336168084042, 0.7520844913841023, 0.6675422138836773, 0.5967119370979271]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8724127587241276, 0.7532496389290079, 0.6687914010748657, 0.5982002571061277]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8724852367130418, 0.7517517517517518, 0.6663746715054436, 0.5949077385209555]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8712970376301041, 0.7512233096085409, 0.6665415415415415, 0.5952517162471396]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8709387183844847, 0.7517494168610463, 0.6676246407597151, 0.5977438240753963]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8708903767362846, 0.7507494171200177, 0.6661671037841889, 0.5956900242614528]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8710063897763578, 0.7514418811002662, 0.6664171656686627, 0.5957810718358039]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8714799281006591, 0.752607055691147, 0.6680808584976291, 0.5980895352152837]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8724510195921631, 0.7532207907596624, 0.6686656671664168, 0.5983723586521987]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8716871687168717, 0.7513057006334037, 0.6663332916614577, 0.5955136448064009]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8735367683841921, 0.7530850472484714, 0.6690431519699812, 0.5987133666904932]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8734493797519007, 0.753223654957759, 0.6687093546773387, 0.5983419096626644]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.872189467372839, 0.7521927389807928, 0.6679155738728613, 0.5975453118310261]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8726273726273727, 0.753496115427303, 0.6695380774032459, 0.5995720399429386]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8723765740555667, 0.7524983344437042, 0.6688733449912565, 0.59905795032829]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8724020783373302, 0.7521092362344582, 0.6678321678321678, 0.5976027397260274]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8715027977617905, 0.7518872113676732, 0.6683316683316683, 0.598601598173516]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8710418539606433, 0.7513039618244368, 0.6674572462863563, 0.59734702610184]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8718256348730254, 0.7518329260164408, 0.6677080729817546, 0.5978291916595259]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8715642178910544, 0.751471404775125, 0.6668332292317302, 0.596716630977873]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8726745349069814, 0.7528339631029117, 0.6686671667916979, 0.5988853958273792]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8726127387261274, 0.7525830463281857, 0.6682914635670542, 0.5983430938437366]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8723127687231277, 0.7523608487945784, 0.6680414948131483, 0.5979145836309099]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8725637181409296, 0.7529150471960022, 0.6685821361648969, 0.5984296930763741]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8726509396241503, 0.7528876055086628, 0.6686656671664168, 0.5988006853226727]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8716769938037178, 0.7514990006662225, 0.6668748438670997, 0.5966314587496432]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8715898870790447, 0.7513045409126236, 0.6665417759460472, 0.5964035964035964]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.871664167916042, 0.7511382565241532, 0.6663335415365397, 0.5960028551034975]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8721127887211279, 0.7514720586601489, 0.6669166354205724, 0.5966290529924296]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8721, 0.7512222222222222, 0.666375, 0.5958571428571429]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.871864067966017, 0.751471404775125, 0.6668332292317302, 0.5965738758029978]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8719896072749076, 0.7515265904296659, 0.6667915573872861, 0.5964035964035964]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8717897471769761, 0.7510824913955813, 0.6662919945048082, 0.5958327386898815]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8717769338396962, 0.7510548523206751, 0.6665001249063203, 0.5960605195546674]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8720511795281887, 0.7515548645046646, 0.6669165417291354, 0.596516276413478]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8715770537677393, 0.7510548523206751, 0.6665001249063203, 0.5962032543534114]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.8716769938037178, 0.751165889407062, 0.6665001249063203, 0.5962032543534114]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.594239241027832, metrics={'train_runtime': 4587.4301, 'train_samples_per_second': 32.698, 'train_steps_per_second': 4.087, 'total_flos': 1357628413575168.0, 'train_loss': 0.594239241027832, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23324fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small-english-to-sql-raw-translation/checkpoint-18600\"\n",
    "model_dir = f\"models/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n",
    "max_input_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5731954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Circuit FROM table WHERE Series = FR3.5 11\n"
     ]
    }
   ],
   "source": [
    "text = \"What circuit was the FR3.5 11 series on?\"\n",
    "\n",
    "inputs = [\"translate English to SQL: \" + text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=4, max_length=128)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_query) # SELECT Circuit FROM table WHERE Series = FR3.5 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70640ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT Language FROM table WHERE Circumstance = 10,748,000\n"
     ]
    }
   ],
   "source": [
    "text = \"How many numbers of languages were there for issues with circulation of 10,748,000?\"\n",
    "\n",
    "inputs = [\"translate English to SQL: \" + text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=4, max_length=128)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_query) # SELECT MAX Number of languages FROM table WHERE Circulation per issue = 10,748,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e35f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT MAX Points FROM table\n"
     ]
    }
   ],
   "source": [
    "text = \"Which team has the greatest number of points?\"\n",
    "\n",
    "inputs = [\"translate English to SQL: \" + text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=4, max_length=128)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dc02b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Opponent FROM table WHERE Week > 7 AND Venue = schaefer stadium\n"
     ]
    }
   ],
   "source": [
    "text = \"Who was the opposing team at Schaefer Stadium later in the season than week 7?   \"\n",
    "\n",
    "inputs = [\"translate English to SQL: \" + text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=4, max_length=128)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_query) # SELECT Opponent FROM table WHERE Week > 7 AND Stadium = schaefer stadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95d2d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Winner FROM table WHERE Grand Prix = australian grand prix\n"
     ]
    }
   ],
   "source": [
    "text = \"Who has won Australian Grand Prix?       \"\n",
    "\n",
    "inputs = [\"translate English to SQL: \" + text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=4, max_length=128)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_query) # SELECT Winning Driver FROM table WHERE Grand Prix = Australian Grand Prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fda7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
